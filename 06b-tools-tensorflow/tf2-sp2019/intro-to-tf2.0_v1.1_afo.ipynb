{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# Data-X: Introduction to TensorFlow 2.0, Tensorboard, and Keras\n",
    "\n",
    "**Author:** Alexander Fred-Ojala\n",
    "\n",
    "**Sources:** Francois Chollet, Sebastian Raschka, Aurélien Géron, etc.\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow:\n",
    "TensorFlow is the most popular and adopted free and open-source deep learning library. It was first developed and maintained by Google. It can be used for both research and production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TensorFlow benefits:**\n",
    "- Highly efficient\n",
    "- Cross-platform (works on IOS, Android, Unix, Windows, in the cloud, in the browser etc etc)\n",
    "- Calculates gradients automatically (this is truly useful for Neural Networks, where the analytical solution of gradients would be VERY tedious to derive).\n",
    "* Deep integration with the Keras library (Functional approach, as well as high-level wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install TensorFlow 2.0\n",
    "\n",
    "TensorFlow 2.x is a major change from TensorFlow 1.x (not backwards compatible, however you can use a tool to convert your TensorFlow 1.x code to 2.x).\n",
    "\n",
    "The new version is designed to be more pythonic. It's  easier to debug models, extract values during training (because of the need of sessions and graphs in TensorFlow 1.x). \n",
    "\n",
    "TensorFlow 2.x supports eager execution by default, so you don't need a session and to evaluate operations / tensors in order to extract values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "# or for GPU version:\n",
    "# !pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical way of importing TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# If this doesn't work TensorFlow is not installed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0\n",
    "At the time of the update of this notebook we are still in the early days of TensorFlow, and currently (Oct 22) the version 2.0.0 has just been released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tf version, oftentimes tensorflow is not backwards compatible\n",
    "tf.__version__\n",
    "\n",
    "# should be tensorflow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "### Core components:\n",
    "\n",
    "#### 1. Tensor\n",
    "A Tensor in TensorFlow is an N-dimensional array (just like Numpys array object). Tensors are multilinear maps from vector spaces to real numbers. Scalars, vectors and matrices are all tensors. The Tensor represents units of data in TensorFlow.\n",
    "\n",
    "Numpy arrays or Pandas DataFrames sent to Tensorflow functions are automatically converted into TensorFlow tensors.\n",
    "\n",
    "#### 2. Operations / Ops\n",
    "TensorFlow operations or ops are units / edges / nodes of computation (e.g. matrix multiplication, addition, etc.)\n",
    "\n",
    "#### 3. Computation Graph\n",
    "The computational graph is is an optimized, compiled representation of the dataflow and the order of computations that are sent to an execution environment (for example during model training).\n",
    "\n",
    "TensforFlow 2.x supports eager execution, but when we build a model and then train it TensorFlow can compile the model and optimize the executions as a computational graph object. This is done by decorating a function with `@tf.function`.\n",
    "\n",
    "This computational graph is then  sent to another instance / runtime environment (e.g. on a CPU or GPU) for execution. The results are sent back to us. This makes TensorFlow computations highly distributable and it also allows us to automatically evaluate all gradients in the computation nodes.\n",
    "\n",
    "![](imgs/tf_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 2.x supports eager execution by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard setup\n",
    "Tip2: Setup TensorBoard if you want to monitor and analyze computational graphs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 tf.constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized directly and eager execution let's us see the values without creating a session and running the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a # note the numpy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .numpy() method will return the result as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eager evaluation of tensors\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also perform operations on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or the same with universal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply(a,b).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_matrix = tf.constant([[1,2], [3,4]])\n",
    "b_matrix = tf.constant([[5,6], [7,8]])\n",
    "b_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(a_matrix, b_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note, we cannot reassign values of constants (like we can with Variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.assign(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 tf.Variable\n",
    "\n",
    "Variables are mutable and can be updated and reassigned new values. Variables are usually weights and biases of a model that are optimized during training, they also indicate the degrees of freedom of the model (what model parameters that can change, thus making the model flexible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable(3.)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the value of a Variable\n",
    "var.assign(4)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create multi dim Variables.\n",
    "d = tf.Variable(np.random.randn(3).reshape(3,1)) #reshape\n",
    "# automatically assings data type\n",
    "d #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace increase / decrease Variable values\n",
    "\n",
    "var.assign(10)\n",
    "print('original value:', var.numpy())\n",
    "print('add 1:', var.assign_add(1.).numpy())\n",
    "print('subtract 5:', var.assign_sub(5.).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables also have a lot of attributes associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[3.,3.2], [1.2,2.2]], dtype=tf.float32, name='my_variable')\n",
    "\n",
    "print('name  : ', v.name)\n",
    "print('type  : ', v.dtype)\n",
    "print('shape : ', v.shape)\n",
    "print('device: ', v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'><b>Note</b>: Tensorflow is really similar to NumPy, and you can think of the tensors as an ndimensional array.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tf_to_np](imgs/tf_to_np.png)\n",
    "Source: CS227d, NLP, Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operations / Ops\n",
    "Operations can be carried out directly or assigned to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op1 = tf.add(a,b)\n",
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a+b # same as tf.add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = a+b\n",
    "u = v+2\n",
    "w = v*u\n",
    "z = w*3\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the computational graph with @tf.function\n",
    "\n",
    "`@tf.function` is a very useful module that can be used to convert simple python functions into a highly optimized computational graph that can be run on any runtime environment. When we build a model and then train it TensorFlow we can compile the model and optimize the executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def func(a,b):\n",
    "    with tf.name_scope('first'):\n",
    "        z = tf.multiply(a,b, name='z')\n",
    "    with tf.name_scope('second'):\n",
    "        y1 = tf.constant(3, name='3')\n",
    "        y2 = tf.constant(4)\n",
    "        w1 = tf.add(z, y1, name='w1')\n",
    "        w2 = tf.add(z, y2, name='w2')\n",
    "        \n",
    "    return(w1+w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a writer to save graph information and TensorFlow logs\n",
    "# To be displayed with Tensorboard\n",
    "\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "tf.summary.trace_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "func(a,b)\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"func\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tensorboard in the shell\n",
    "!tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.function and Conditional statements\n",
    "It is difficult to use conditions in graphs but we could implement that easily using `@tf.function` decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def g(x):\n",
    "    y = tf.reduce_sum(x)\n",
    "    if y > 0:\n",
    "        return y\n",
    "    return tf.abs(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.autograph.to_code(g.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient evaluation is very importnat machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(3.0)\n",
    "\n",
    "# Gradient scope for the function w^2\n",
    "with tf.GradientTape() as tape:\n",
    "    square_w = w * w\n",
    "\n",
    "grad = tape.gradient(square_w, w)\n",
    "print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the Sigmoid function\n",
    "In this example we evaluate the gradient of the sigmoid function \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Note that \n",
    "\n",
    "$$\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x)(1-\\sigma(x)) $$\n",
    "\n",
    "For instance \n",
    "\n",
    "$$\\sigma'(0) = \\sigma(0)(1-\\sigma(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2} \\right) = \\frac{1}{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + tf.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a varaible\n",
    "x = tf.Variable(0.)\n",
    "\n",
    "#record the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    sig = sigmoid(x)\n",
    "    \n",
    "res = tape.gradient(sig, x).numpy()\n",
    "print('The gradient of the sigmoid function at 0.0 is ', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in TensorFlow\n",
    "\n",
    "This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n",
    "\n",
    "$$y = f(x) + noise = 3 x + 2 + noise$$\n",
    "\n",
    "Given a point $x$ we want to predict the value of $f(x)$. We train the model on 100 data pairs $(x,y)$. \n",
    "\n",
    "We want the model to learn a linear model \n",
    "\n",
    "$$\\hat{y} = W x + b$$\n",
    "\n",
    "Note that, we use `tf.GradientTape` to record the gradient of the loss function with respect our model paramters.  \n",
    "\n",
    "We use MSE to calcuate the loss \n",
    "\n",
    "$$MSE = \\frac{1}{100} (y-\\hat{y})^2$$\n",
    "\n",
    "We use Gradient Descent to update the paramters \n",
    "\n",
    "$$W = W - \\alpha  \\frac{\\partial MSE}{\\partial W}$$\n",
    "\n",
    "$$b = b - \\alpha  \\frac{\\partial MSE}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 data points \n",
    "NUM_EXAMPLES = 100\n",
    "\n",
    "#define inputs and outputs with some noise \n",
    "X = tf.random.normal([NUM_EXAMPLES])  #inputs \n",
    "noise = tf.random.normal([NUM_EXAMPLES]) #noise \n",
    "y = X * 3 + 2 + noise  #true output\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model paramters with initial values \n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training info\n",
    "train_steps = 300\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.trace_on() # trace graph\n",
    "\n",
    "\n",
    "#watch the gradient flow\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward pass \n",
    "        yhat = X * W + b\n",
    "\n",
    "        #calcuate the loss (difference squared error)\n",
    "        error = yhat - y\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "    #evalute the gradient with the respect to the paramters\n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "\n",
    "    #update the paramters using Gradient Descent  \n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db* learning_rate)\n",
    "\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the loss every 20 iterations\n",
    "for i in range(train_steps):\n",
    "    loss = train_on_batch(X,y)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step=i)\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
    "        \n",
    "        \n",
    "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_on_batch(X,y)\n",
    "with writer.as_default():\n",
    "        tf.summary.trace_export(\n",
    "        name=\"linreg\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, b+W*X, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Neural Network with TensorFlow and Keras\n",
    "\n",
    "These tutorials use `tf.keras`, TensorFlow's high-level Python API for building and training deep learning models. To learn more about using Keras with TensorFlow, see the TensorFlow Keras Guide.\n",
    "\n",
    "TensorFlow 2.x has integrated Keras and all it's functionality. Import all functionality from `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Keras\n",
    "Modular, powerful and intuitive Deep Learning python library built on TensorFlow, CNTK, Theano.\n",
    "* Minimalist, user-friendly interface\n",
    "* Modular\n",
    "* Deep integration with Tensorflow (`tf.keras`)\n",
    "* Works on CPUs and GPUs\n",
    "* Open-source, developed and maintained by a community of contributors, and\n",
    "publicly hosted on github\n",
    "* Extremely well documented, lots of working examples: https://keras.io/\n",
    "* Very shallow learning curve —> it is by far one of the best tools for experimenting, both for beginners and experts\n",
    "* Easy to extend\n",
    "* Similar to scikit-learn's API and syntax calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras \"Hello World\" on Iris\n",
    "\n",
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = datasets.load_iris()\n",
    "\n",
    "print(data.DESCR[:1011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode y\n",
    "import pandas as pd\n",
    "\n",
    "y = pd.get_dummies(y).values\n",
    "y[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split, plus randomize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, test_size=0.4,\n",
    "                                                    random_state=1337,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sequential model\n",
    "The simplest model in Keras is the Sequential model, a linear stack of layers. In Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers: the `tf.keras.Sequential` model.\n",
    "\n",
    "To build a simple, fully-connected network (i.e. multi-layer perceptron):\n",
    "\n",
    "* **Sequential model** Allows us to build NNs like legos, by adding one layer on top of the other, and swapping layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data structure in Keras is a model\n",
    "# The model is an object in which we organize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential() # instantiate empty Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import layer classes and stack layers (in an NN model for example), by using `.add()`\n",
    "\n",
    "# Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a  Sequential model needs to receive information about its input shape.\n",
    "\n",
    "**The following snippets are strictly equivalent:**\n",
    "> * `model.add(Dense(32, input_shape=(784,)))`\n",
    "> * `model.add(Dense(32, input_dim=784))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model contruction (architecture build computational graph)\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add( Dense(units=64, activation='relu', \\\n",
    "                 input_shape=(4,) ))\n",
    "\n",
    "model.add( Dense(units=3, activation='softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation phase, specify learning process\n",
    "\n",
    "Run `.compile()` on the model to specify learning process.\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the  compile method. It receives three arguments:\n",
    "\n",
    "* **A loss function:** This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function.\n",
    "* **An optimizer:** This could be the string identifier of an existing optimizer (such as `rmsprop`, `gradientdescent`, or `adam`), or an instance of the Optimizer class.\n",
    "* **(Optional) A list of metrics:** For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also specify our own optimizer or loss function (even build it ourselves)\n",
    "\n",
    "```python\n",
    "# or with we can specify loss function or optimizer\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = SGD(lr=0.001, momentum = 0.9, nesterov=True),\n",
    "             metrics = ['accuracy'])\n",
    "```\n",
    "\n",
    "### Different optimizers and their trade-offs\n",
    "To read more about gradient descent optimizers, hyperparameters etc. This is a recommended reading: http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model by iterating over the training data in batches\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "model.evaluate(X_test, y_test,verbose=False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on new data:\n",
    "\n",
    "class_probabilities = model.predict(X_test)\n",
    "\n",
    "# gives output of the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probabilities[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "The upcoming section will train neural networks / multilayered perceptrons. Make sure you're familiar with these concepts (activation functions, backpropogation, optimizers, softmax etc). Check the presentations in the Github folder.\n",
    "\n",
    "**Also watch 3Blue1Brown's fantastic animation on how DNNs are trained and how they classify:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aircAruvnKk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aircAruvnKk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Intro to NN in TensorFlow\n",
    "\n",
    "Example taken from Google Docs\n",
    "\n",
    "We are now going to recognize hand-written digits.\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST.png](https://www.tensorflow.org/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the most classic NN dataset\n",
    "The MNIST data is split into three parts: 60,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test).\n",
    "\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST-Matrix.png](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a very rich structure (warning: computationally intensive visualizations).\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [60000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![https://www.tensorflow.org/images/mnist-train-xs.png](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n",
    "                            datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input information\n",
    "print('Train input shape:',x_train.shape)\n",
    "print('Test input shape:',x_test.shape)\n",
    "print('Input data type:',x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min-max values:',np.min(x_train),np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image:')\n",
    "plt.imshow(x_train[1],cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output information:\n",
    "print('Train output shape:',y_train.shape)\n",
    "print('Test output shape:',y_test.shape)\n",
    "print('Data type:',y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique labels:',np.unique(y_train))\n",
    "print('First 10 outputs:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, flatten inputs, and convert datatype\n",
    "x_train = x_train.reshape(60000, 784). \\\n",
    "                    astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape(10000, 784) \\\n",
    "                    .astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf $logdir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load standard NN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model constructor\n",
    "model = Sequential()\n",
    "# Add layers sequentially\n",
    "model.add(Dense(300, activation='relu', \\\n",
    "                    input_shape=(784,)))\n",
    "\n",
    "# Second..\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "# Third..\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# train the model\n",
    "\n",
    "NO_EPOCHS = 5\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=NO_EPOCHS,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(range(NO_EPOCHS),history.history['val_accuracy'],c='g',label='validation acc')\n",
    "plt.plot(range(NO_EPOCHS),history.history['accuracy'],c='r',label='training acc')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Vanilla) ANN Network structure\n",
    "* Any Neural Network with one hidden layer can be a Universal Function Approximator. Source: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* The number of input nodes are equal to the number of features\n",
    "* The number of output nodes are equal to the number of classes (for classification tasks)\n",
    "* A bias term is added to every layer that only feeds in a 1, that adds an extra degree of freedom for every functional input value to the next function\n",
    "* For the input layer we send in the (standardized) values.\n",
    "* For the output layer we often use a softmax function (multi-class classification) or a sigmoid function (binary classification). Softmax only works if the classes are mutually exclusive, i.e. we only try to label one pattern in every training example.\n",
    "\n",
    "# How deep should we go?\n",
    "* We can overfit Neural Nets, one way to combat that is by using dropout and regularization\n",
    "* Predictions will usually be better when we increase depth of network and widen it (increase the number of neurons in every layer)\n",
    "\n",
    "# Activation Functions\n",
    "* Classically the sigmoid function was used in the hidden layers (simplest function between 0 - 1). Logit function.\n",
    "* Nowadays it is more common to use the ReLU (Rectified Linear Unit). Much quicker! For deep networks sigmoid might not want to converge at all. Much better to handle exploding and vanishing gradients (Leaky Relu). Can also combat that with *Batch Normalization*.\n",
    "\n",
    "\n",
    "# Training algorithm steps\n",
    "* Train a model to make a prediction\n",
    "* Compute distance between predictions and true values\n",
    "* Modify weights and biases to lower error\n",
    "\n",
    "\n",
    "# Overfitting\n",
    "* Mostly because our network has too many degrees of freedom (neurons in the network)\n",
    "* Can use L1 and L2 regularization on the cost function\n",
    "* Drop out (used to mitigate the effects of too many degrees of freedom)\n",
    "\n",
    "# ANNs are not great at classifying images\n",
    "* We don't make use of the image shapes and curves. Shape info is lost when we flatten arrays.\n",
    "\n",
    "# ANN One Layer Softmax Classification\n",
    "\n",
    "What we accomplished in this section:\n",
    "\n",
    "- Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n",
    "- Check the model's accuracy with our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place tensors on the CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x, y):\n",
    "    #normalize and expand\n",
    "    x = tf.cast(x, tf.float32)/255.\n",
    "    x = tf.expand_dims(x, -1)\n",
    "\n",
    "    #cast the labels\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(x, y):  \n",
    "    #convert to tensors and shuffle\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x , y)).shuffle(len(x)-1)\n",
    "\n",
    "    #extract batches\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    #preprocess the batch\n",
    "    dataset = dataset.map(pre_process, num_parallel_calls = 4)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data (flatten input, convert to floating point and normalize)\n",
    "# Then convert to Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=60000)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches\n",
    "dataset = dataset.batch(64)\n",
    "dataset # creates a new dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tf2 model training\n",
    "\n",
    "Work in progress (porting to tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and input size\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neuron layers (ReLU in hidden layers)\n",
    "# We'll take care of Softmax for output with loss function\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    # X input to neuron\n",
    "    # number of neurons for the layer\n",
    "    # name of layer\n",
    "    # pass in eventual activation function\n",
    "    n_inputs = int(X.shape[1])\n",
    "\n",
    "    # initialize weights to prevent vanishing / exploding gradients\n",
    "    stddev = 2 / np.sqrt(n_inputs)\n",
    "    init = tf.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "    # Initialize weights for the layer\n",
    "    W = tf.Variable(init((n_inputs, n_neurons)), name=\"weights\")\n",
    "    # biases\n",
    "    b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "\n",
    "    # Output from every neuron\n",
    "    Z = tf.matmul(X, W) + b\n",
    "    if activation is not None:\n",
    "        return activation(Z)\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = neuron_layer(x_train, n_hidden1, name=\"hidden1\",\n",
    "                       activation=tf.nn.relu)\n",
    "hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                       activation=tf.nn.relu)\n",
    "logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step with Gradient Descent\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (that also optimizes Softmax for output):\n",
    "\n",
    "\n",
    "# logits are from the last output of the dnn\n",
    "xentropy = tf.keras.losses.sparse_categorical_crossentropy(y_train, logits, from_logits=True)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(x, label):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(x)\n",
    "#         loss = loss_object(label, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# \n",
    "#     train_loss(loss)\n",
    "#     train_accuracy(label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-x",
   "language": "python",
   "name": "data-x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
